{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def ReLU_Derivative(X):\n",
    "    return X > 0\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, numInput, numHidden):\n",
    "        self.W1 = np.random.rand(numInput, numHidden)*2-1\n",
    "        self.b1 = np.random.rand(numHidden)*2-1\n",
    "        self.W2 = np.random.rand(numHidden, 1)*2-1\n",
    "        self.b2 = np.random.rand(1)*2-1\n",
    "        \n",
    "    def feedForward(self, X):\n",
    "        net2 = self.W1.T @ X.T + self.b1[:,np.newaxis]\n",
    "        o2 = ReLU(net2) #shape (20,10)\n",
    "        net3 = self.W2.T @ o2 + self.b2[:,np.newaxis]\n",
    "        return net3\n",
    "    \n",
    "    def backprop(self, X, Y):\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        net2 = self.W1.T @ X.T + self.b1[:,np.newaxis]\n",
    "        o2 = ReLU(net2) #shape (20,10)\n",
    "        net3 = self.W2.T @ o2 + self.b2[:,np.newaxis]\n",
    "        \n",
    "        delta2 = 2*(net3 - Y) #shape (1,10)\n",
    "    \n",
    "        \n",
    "        self.b2_gradient = delta2 #shape (10, )\n",
    "        self.W2_gradient = o2 @ delta2.T #shape (20,10)\n",
    "\n",
    "        #self.W2 shape (20,1)\n",
    "        delta1 = self.W2 @ delta2 #shape (20,10)\n",
    "        \n",
    "        o2_deriv = ReLU_Derivative(net2) #shape (20,10)\n",
    "        \n",
    "        self.b1_gradient = delta1 * o2_deriv #shape (20,10)\n",
    "        self.W1_gradient = X.T @ (self.b1_gradient).T #shape (2,10)*(10,20)=(2,20)\n",
    "        \n",
    "    def draw(self, X, Y, K):\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        indices = np.random.choice(X.shape[0], K, replace=False)\n",
    "        return X[indices], Y[indices]\n",
    "    \n",
    "    def fit(self, X, Y, nu, K, numIterations):\n",
    "        \n",
    "        for i in range(numIterations):\n",
    "            X_batch, Y_batch = self.draw(X, Y, K)\n",
    "            self.backprop(X_batch, Y_batch)\n",
    "            self.W1 -= nu*self.W1_gradient\n",
    "            self.b1 -= nu*np.sum(self.b1_gradient, axis = 1)\n",
    "            self.W2 -= nu*self.W2_gradient\n",
    "            self.b2 -= nu*np.sum(self.b2_gradient, axis = 1)\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                diff = self.feedForward(X)-Y\n",
    "                print(\"The lse is \", np.inner(diff, diff))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-2-0c7fe8e3c8fd>(23)backprop()\n",
      "-> net2 = self.W1.T @ X.T + self.b1[:,np.newaxis]\n",
      "(Pdb) p X\n",
      "array([[ 0.61144722, -1.34583753],\n",
      "       [-0.06860615, -1.81919179],\n",
      "       [ 0.47370503, -0.58351456]])\n",
      "(Pdb) p b1\n",
      "*** NameError: name 'b1' is not defined\n",
      "(Pdb) p self.b1\n",
      "array([-0.346358  , -0.84264916,  0.24298109, -0.17736757,  0.6178669 ,\n",
      "        0.7064968 ,  0.69033168, -0.75900246,  0.0834226 ,  0.0271218 ])\n",
      "(Pdb) p self.W1\n",
      "array([[ 0.99450778, -0.15326251, -0.13191761,  0.735622  , -0.82196524,\n",
      "         0.36295685, -0.38532054, -0.99992743, -0.01404396,  0.54992522],\n",
      "       [ 0.20586394, -0.63450806, -0.02027325, -0.90641104,  0.94732219,\n",
      "        -0.56913809, -0.34341476, -0.52741874, -0.88049832, -0.32171509]])\n",
      "(Pdb) n\n",
      "> <ipython-input-2-0c7fe8e3c8fd>(24)backprop()\n",
      "-> o2 = ReLU(net2) #shape (20,10)\n",
      "(Pdb) p net2\n",
      "array([[-0.0153284 , -0.78909334,  0.00462073],\n",
      "       [-0.08241633,  0.32215744, -0.54500569],\n",
      "       [ 0.18960494,  0.28891239,  0.1923208 ],\n",
      "       [ 1.49230846,  1.42109977,  0.70000432],\n",
      "       [-1.15966321, -1.04910197, -0.32427845],\n",
      "       [ 1.69439316,  1.71696706,  1.21053165],\n",
      "       [ 0.91690898,  1.34150435,  0.70819091],\n",
      "       [-0.66058538,  0.26907455, -0.92491661],\n",
      "       [ 1.25984314,  1.68618141,  0.59055349],\n",
      "       [ 0.79634829,  0.574655  ,  0.47534958]])\n"
     ]
    }
   ],
   "source": [
    "b = np.random.multivariate_normal(np.zeros(2), np.eye(2), 10000) \n",
    "u = np.power(np.random.rand(10000)*4, 0.5)\n",
    "b = normalize(b) * u.reshape(u.size,1)\n",
    "norms = np.linalg.norm(b, axis = 1)\n",
    "x = np.append(b[norms < 1][:250], b[norms > 1][:250], axis = 0)\n",
    "y = np.append(-np.ones(250), np.ones(250))\n",
    "\n",
    "plt.scatter(x[:250,0],x[:250,1],c='orange')\n",
    "plt.scatter(x[250:,0],x[250:,1],c='blue')\n",
    "\n",
    "clf = TwoLayerNN(2,10)\n",
    "clf.fit(x,y,0.1,3,50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
