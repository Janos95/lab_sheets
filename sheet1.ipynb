{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "irisDataFrame = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_a = np.array([1.5, 0])\n",
    "\n",
    "cov = np.eye(2)\n",
    "\n",
    "a = np.random.multivariate_normal(mean_a,cov,10)\n",
    "\n",
    "plt.scatter(a[:,0],a[:,1], c='orange')\n",
    "\n",
    "b = np.random.multivariate_normal(mean_a[::-1],cov,20)\n",
    "\n",
    "plt.scatter(b[:,0],b[:,1],c='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0,10,200)\n",
    "noise = np.random.multivariate_normal(np.zeros(2),0.25*cov,200)\n",
    "x = np.append(a[indices[:100]], b[indices[100:]], 0) + noise \n",
    "y = np.append(np.zeros(100), np.ones(100))\n",
    "\n",
    "plt.scatter(x[:100,0], x[:100,1],c='orange')\n",
    "\n",
    "plt.scatter(x[100:,0], x[100:,1],c='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "#this function solves the linear least squares problem using a qr factorization.\n",
    "#If x and y are nxk and nx1 arrays then alpha is an \n",
    "def lls(x,y):\n",
    "    X_hat = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "    b = np.dot(X_hat.T, y)\n",
    "    q, r = LA.qr(np.dot(X_hat.T, X_hat))\n",
    "    p = np.dot(q.T, b)\n",
    "    alpha = np.dot(LA.inv(r), p)\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies linear map alpha to x\n",
    "#Values x have to be passed as n times 2 array\n",
    "def apply(alpha,x):\n",
    "    x_hat = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "    return np.dot(x_hat, alpha)\n",
    "\n",
    "def PlotContourLine(func, value=0, minx=0, maxx=10, miny=0, maxy=10):\n",
    "    #This plots the contourline func(x) = value\n",
    "    \n",
    "    samplenum = 1000\n",
    "    xrange = np.arange(minx, maxx, (maxx-minx)/samplenum)\n",
    "    yrange = np.arange(miny, maxy, (maxy-miny)/samplenum)\n",
    "    \n",
    "    #This generates a two-dimensional mesh\n",
    "    X, Y = np.meshgrid(xrange,yrange)\n",
    "    \n",
    "    argsForf = np.array([X.flatten(),Y.flatten()]).T\n",
    "    Z = func(argsForf)\n",
    "    Z = np.reshape(Z,X.shape)\n",
    "    \n",
    "    plt.xlim(minx, maxx)\n",
    "    plt.ylim(miny, maxy)\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    #plt.contour(X, Y, Z, alpha=0.5,levels=[value],linestyles='dashed',linewidths=3)\n",
    "    Z = np.where(Z > value, 1, -1)\n",
    "    plt.contourf(X, Y, Z, alpha=0.2, colors=('red', 'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = lls(x,y)\n",
    "f = lambda x: apply(alpha,x)\n",
    "\n",
    "PlotContourLine(f, 0.5, -3.0, 4.0, -3.0, 4.0)\n",
    "\n",
    "plt.scatter(x[:100,0], x[:100,1],c='orange')\n",
    "\n",
    "plt.scatter(x[100:,0], x[100:,1],c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the vector y is the ground truth for the test points x. \n",
    "#In particular we need the length of y to be the same as the length of x.\n",
    "def compute_confusion_matrix(f, x, y, n):\n",
    "    c = np.empty((n,n))\n",
    "    y_predicted = np.rint(np.clip(f(x),0,1).astype(np.float))\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            c[i,j] = np.sum(np.logical_and(y_predicted == i, y == j))\n",
    "    return c\n",
    "\n",
    "c = compute_confusion_matrix(f,x,y,2)\n",
    "print(\"The confusion matrix for our training data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/x.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0,10,20000)\n",
    "noise = np.random.multivariate_normal(np.zeros(2),0.25*cov,20000)\n",
    "\n",
    "x_test = np.append(a[indices[:10000]], b[indices[10000:]], 0) + noise \n",
    "y_test = np.append(np.zeros(10000), np.ones(10000))\n",
    "\n",
    "c_test = compute_confusion_matrix(x_test, y_test, 2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c_test)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c_test)/x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model decreased on the test set as expected, since\n",
    "by construction the seperating hyperplane minimizes the error only on the training set.\n",
    "One could say that our model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "#irisDataFrame = pd.read_csv(url, header=None)\n",
    "\n",
    "x = irisDataFrame.values[:,:2]\n",
    "y = irisDataFrame.values[:,4] != 'Iris-setosa'\n",
    "\n",
    "alpha = lls(x,y)\n",
    "f = lambda x: apply(alpha,x)\n",
    "\n",
    "num_setosa = np.where(y==1)[0][0]\n",
    "\n",
    "PlotContourLine(f,0.5,4,8,1,5)\n",
    "plt.scatter(x[:num_setosa,0], x[:num_setosa,1],c='orange')\n",
    "plt.scatter(x[num_setosa:,0], x[num_setosa:,1],c='blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = irisDataFrame.values[:,:4]\n",
    "alpha = lls(x,y)\n",
    "f = lambda x: apply(alpha,x)\n",
    "\n",
    "c = compute_confusion_matrix(x, y, 2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/x.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = irisDataFrame.values[:,:2]\n",
    "y = irisDataFrame.values[:,4] != 'Iris-versicolor'\n",
    "\n",
    "alpha = lls(x,y)\n",
    "f = lambda x: apply(alpha,x)\n",
    "\n",
    "PlotContourLine(f,0.5,4,8,1,5)\n",
    "plt.scatter(x[:50,0], x[:50,1],c='orange')\n",
    "plt.scatter(x[50:100,0], x[50:100,1],c='blue')\n",
    "plt.scatter(x[100:150,0], x[100:150,1],c='orange')\n",
    "\n",
    "x = irisDataFrame.values[:,:4]\n",
    "alpha = lls(x,y)\n",
    "f = lambda x: apply(alpha,x)\n",
    "\n",
    "c = compute_confusion_matrix(x, y,2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/x.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not linearly separable anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA \n",
    "\n",
    "\n",
    "def grad_decent(x,y, alpha):\n",
    "    errors = np.zeros(100)\n",
    "#    alpha = np.random.randn(x.shape[1]+1)\n",
    "    x_hat = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "    for i in range(0,100):\n",
    "        y_old = np.dot(x_hat, alpha)\n",
    "        alpha = alpha - nu * 2.0/150 * np.dot(x_hat.T, y_old-y)\n",
    "        y_new = np.dot(x_hat, alpha)\n",
    "        errors[i] = LA.norm(y_new-y)\n",
    "        #if the error termin converges, exit the look\n",
    "#         if LA.norm(y_new-y_old) < 1e-3:\n",
    "#             print(\"The error term converged to {} after {} steps\".format(errors[-1],i))\n",
    "#             break\n",
    "    return (alpha, errors)\n",
    "\n",
    "x = irisDataFrame.values[:,:4]\n",
    "y = irisDataFrame.values[:,4] != 'Iris-versicolor'\n",
    "\n",
    "nu = 1e-2\n",
    "alpha = np.random.randn(x.shape[1]+1)\n",
    "(alpha, errors) = grad_decent(x,y,alpha)\n",
    "\n",
    "plt.scatter(range(0,len(errors)),errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is very important. It needs to be carfeully set, otherwise gradient decent will not converge to a global minimum.\n",
    "There seem to be more local minima if we try to classify setosa vs rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "mean = np.sum(x,0)/x.shape[0]\n",
    "print(mean)\n",
    "std = LA.norm((x-mean).astype(np.double),axis=0)/np.sqrt(x.shape[0])\n",
    "print(std)\n",
    "x_n = (x-mean)/std\n",
    "print(x_n)\n",
    "\n",
    "errors_unnormalized = np.zeros(100)\n",
    "errors_normalized = np.zeros(100)\n",
    "for i in range(0,100):\n",
    "    alpha = np.random.rand(x.shape[1]+1)\n",
    "    (_, errors) = grad_decent(x,y,alpha)\n",
    "    errors_unnormalized = errors_unnormalized + errors\n",
    "    (_, errors) = grad_decent(x_n,y,alpha)\n",
    "    errors_normalized = errors_normalized + errors\n",
    "    \n",
    "\n",
    "errors_unnormalized = errors_unnormalized / 100\n",
    "errors_normalized = errors_normalized / 100\n",
    "\n",
    "plt.scatter(range(0,100),errors_unnormalized, marker='x')\n",
    "plt.scatter(range(0,100),errors_normalized, c='red', marker='x')\n",
    "\n",
    "print(\"The average error after 100 iterations for the unnormalized data is \", errors_unnormalized[-1])\n",
    "print(\"The average error after 100 iterations for the normalized data is \", errors_normalized[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better comparability we run gradient decent a 100 times for each the normalized training set and the unnormalized training set. I then average the errors to obtain the above plot. I can't tell any advantage in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexandros test\n",
    "\n",
    "mean_a = np.array([1.5, 0])\n",
    "cov = np.eye(2)\n",
    "a = np.random.multivariate_normal(mean_a,cov,10)\n",
    "b = np.random.multivariate_normal(mean_a[::-1],cov,20)\n",
    "\n",
    "indices = np.random.randint(0,10,200)\n",
    "noise = np.random.multivariate_normal(np.zeros(2),0.25*cov,200)\n",
    "x = np.append(a[indices[:100]], b[indices[100:]], 0) + noise \n",
    "y = np.append(np.zeros(100), np.ones(100))\n",
    "\n",
    "plt.scatter(x[:100,0], x[:100,1],c='orange')\n",
    "plt.scatter(x[100:,0], x[100:,1],c='blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "\n",
    "d=dist.cdist(x,x,'euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "a = np.random.multivariate_normal(mean_a,cov,10)\n",
    "b = np.random.multivariate_normal(mean_a[::-1],cov,20)\n",
    "indices = np.random.randint(0,10,200)\n",
    "noise = np.random.multivariate_normal(np.zeros(2),0.25*cov,200)\n",
    "x = np.append(a[indices[:100]], b[indices[100:]], 0) + noise \n",
    "y = np.append(np.zeros(100), np.ones(100))\n",
    "indices = np.random.randint(0,10,20000)\n",
    "noise = np.random.multivariate_normal(np.zeros(2),0.25*cov,20000)\n",
    "x_test = np.append(a[indices[:10000]], b[indices[10000:]], 0) + noise \n",
    "y_test = np.append(np.zeros(10000), np.ones(10000))\n",
    "\n",
    "def k_nearest_neighbors(points,k):\n",
    "    return np.sum(y[np.argsort(distance.cdist(points,x))[:,:k]],1)/k\n",
    "\n",
    "f = lambda x: k_nearest_neighbors(x, 1)\n",
    "c = compute_confusion_matrix(x, y, 2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/c.shape[0])\n",
    "\n",
    "\n",
    "PlotContourLine(f, 0.5, -3.0, 4.0, -3.0, 4.0)\n",
    "\n",
    "plt.scatter(x_test[:10000,0], x_test[:10000,1],c='orange')\n",
    "\n",
    "plt.scatter(x_test[10000:,0], x_test[10000:,1],c='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: k_nearest_neighbors(x, 15)\n",
    "c = compute_confusion_matrix(x, y, 2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/c.shape[0])\n",
    "\n",
    "f = lambda x: k_nearest_neighbors(x, 185)\n",
    "c = compute_confusion_matrix(x, y, 2)\n",
    "print(\"The confusion matrix for our test data is:\\n\" ,c)\n",
    "print(\"The accuracy of the model is: \",  np.trace(c)/c.shape[0])\n",
    "\n",
    "PlotContourLine(f, 0.5, -3.0, 4.0, -3.0, 4.0)\n",
    "\n",
    "plt.scatter(x[:100,0], x[:100,1],c='orange')\n",
    "\n",
    "plt.scatter(x[100:,0], x[100:,1],c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(199)\n",
    "errors_test = np.zeros(199)\n",
    "\n",
    "for i in range(1,200):\n",
    "    f = lambda x: k_nearest_neighbors(x, i)\n",
    "    c = compute_confusion_matrix(f, x_test, y_test, 2)\n",
    "    errors[i-1] = np.trace(c)/2.0\n",
    "    \n",
    "    c = compute_confusion_matrix(f, x_test, y_test, 2)\n",
    "    errors_test[i-1] = np.trace(c)/200.0\n",
    "    \n",
    "plt.scatter(range(1,200),errors, marker='x')\n",
    "plt.scatter(range(1,200),errors_test, c='red', marker='x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
